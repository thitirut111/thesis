import torch
import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    Trainer, 
    TrainingArguments, 
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
model_name = "mistralai/Mistral-7B-Instruct-v0.3"

# 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö 4-bit (Quantization)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# 3. ‡πÇ‡∏´‡∏•‡∏î Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    use_fast=False
)
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Pad Token (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mistral)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" 

# 4. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# 5. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Low-Bit Training (QLoRA)
model.config.use_cache = False # ‡∏õ‡∏¥‡∏î cache ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
model.config.pad_token_id = tokenizer.eos_token_id
model = prepare_model_for_kbit_training(model)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA Config
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"], # ‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Mistral
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)

print("‚úÖ Model & Tokenizer loaded with QLoRA successfully")

# 6. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSONL
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° Path ‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ
train_file = r"C:\Users\Dhinotea\Downloads\thesis\train.jsonl"
test_file = r"C:\Users\Dhinotea\Downloads\thesis\test.jsonl"
valid_file = r"C:\Users\Dhinotea\Downloads\thesis\val.jsonl"

dataset = load_dataset('json', data_files={
    'train': train_file,
    'test': test_file,
    'validation': valid_file
})

# 7. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSONL ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Column ‡∏ä‡∏∑‡πà‡∏≠ 'raw_input'
def tokenize_function(examples):
    outputs = tokenizer(
        examples['raw_input'], 
        padding="max_length", 
        truncation=True, 
        max_length=512 # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì
    )
    outputs["labels"] = outputs["input_ids"].copy() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Causal LM labels ‡∏Ñ‡∏∑‡∏≠ input_ids
    return outputs

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset["train"].column_names)

# 8. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å (Training Arguments)
training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=2,
    per_device_train_batch_size=2, # ‡∏ñ‡πâ‡∏≤ VRAM ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 4
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True, # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM
    learning_rate=1e-4,
    optim="paged_adamw_32bit", # Optimizer ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö QLoRA
    report_to="none" # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ wandb/tensorboard ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ
)

# 9. ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
    tokenizer=tokenizer,
)

print("üöÄ Starting training...")
trainer.train()

# 10. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà Train ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
model.save_pretrained("./mistral-finetuned-thesis")
tokenizer.save_pretrained("./mistral-finetuned-thesis")

# 11. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Test
print("üìä Evaluating on test set...")
trainer.evaluate(tokenized_datasets['test'])
