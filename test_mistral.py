import torch
import json
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Path (‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏∞)
base_model_path = "mistralai/Mistral-7B-Instruct-v0.3"
adapter_path = r"C:\Users\Dhinotea\mistal_train\mistral-finetuned-thesis" 
test_file = r"C:\Users\Dhinotea\Downloads\thesis\test.jsonl"

# 2. ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model (‡πÉ‡∏ä‡πâ 4-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=bnb_config,
    device_map="auto"
)

# ‡πÇ‡∏´‡∏•‡∏î "‡∏™‡∏°‡∏≠‡∏á" ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏™‡∏ß‡∏°‡∏ó‡∏±‡∏ö
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval() 

print("‚úÖ Loaded Fine-tuned Model Successfully!")

# 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Test ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö
results = []
with open(test_file, 'r', encoding='utf-8') as f:
    for line in f:
        data = json.loads(line)
        prompt = data['raw_input']
        ground_truth = data.get('label', 'N/A')
        reason_truth = data.get('reason', 'N/A')

        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ AI
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=100, # ‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100 ‡∏Ñ‡∏≥
                temperature=0.1     # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ß
            )
        
        # ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà AI ‡∏ï‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # (Optional) ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏≠‡∏≠‡∏Å ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        ai_response_only = answer.replace(prompt, "").strip()

        results.append({
            "Input": prompt,
            "Ground_Truth_Label": ground_truth,
            "Ground_Truth_Reason": reason_truth,
            "AI_Prediction": ai_response_only
        })
        print(f"Processed: {len(results)}")

# 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á CSV ‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô Excel
df = pd.DataFrame(results)
df.to_csv("my_thesis_test_results.csv", index=False, encoding='utf-8-sig')
print("üèÅ Done! ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå my_thesis_test_results.csv ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö")